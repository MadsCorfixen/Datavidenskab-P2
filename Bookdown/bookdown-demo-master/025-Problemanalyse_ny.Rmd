```{r include=FALSE, MESSAGE = FALSE}
library(mosaic)
```

__FIXME__ (Afsnittet skal kigges igennem, mhp. ILT! Noget af indledening til afsnittet minder om problemanalysen)

# Problemanalyse

## Statistikkens udvikling

Ordet _statistik_ stammer tilbage fra det latinske _statisticum collegium_ ("statsrådgiver") og det italienske _statista_ ("statsmand" eller "politiker"), [@Orddef]. At statistik netop stammer derfra, giver god mening, under anskueelse af betydningen af disse to ord og den tidlige anvendelse af statistik. I takt med udviklingen af suverænitetsstaterne, steg behovet for at registrere befolkningen og dennes tilhørselsforhold. Derfor anvendte statsrådgivere og statsmænd statistik til at beskrive staten, særligt demografien. Senerere blev dette udvidet til at indsamle flere informationer, og ligeledes at analysere og fortolke disse ved hjælp af statistik.

To statistikere, som ligger fundamentet til den statistiske arbejdsform, der bruges i dag er Karl Pearson og Ronald Fisher.
Karl Pearson var interesseret i at udvikle matematiske metoder til at studere biologisk arv og evolution. Gennem den interesse udsprang en række bidrag til statistiske analyse metoder. Pearson opfandt korrelationskoefficienten $(R^2)$ som bruges til at vise, hvor godt en regressionsmodel passer noget givent data. Udover det, opfandt han også $\chi^2$-testen, som er en metode der bruges til at teste at observerede værdier stemmer overens med de forventede værdier, [@KarlPearsonbrit], [@Chiianden].

Ronald Fisher introducerede princippet om randomisering. Det siger, at et eksperiment skal gentages på et antal kontrolgrupper, og de elementer der bruges i eksperimentet skal tilfældigt udvælges fra hele populationen. Dette gjorde data forventningsret, som mindsker variationen i et eksperiment. Udover det har Fisher introduceret analyse af varians, også kaldet ANOVA (fra engelsk analysis of variance). Denne model bruges til at analysere forskellen mellem gruppemiddelværdier i en stikprøve, [@RonaldFisher].

Yderligere har udviklingen af computeren været med til at gøre anvendelsen af komplicerede statistiske beregninger hurtigere, mere præcise og mere tilgængelige. Anvendelsesområderne for statistik har ligeledes udviklet sig, fra i begyndelsen at være noget staten anvendte til styring af økonomi og befolkningsindblik, til stort set at være repræsenteret i alle større hverv i dag. Den moderne definition af statistik kan beskrives som evnen til at drage konklusioner om generelle tilfælde, _populationer_, på baggrund af enkelte tilfælde, _stikprøver_, [@ASTAbog, s. 1].

## Statistiksk inferens

Det følgende afsnit er baseret på [@ASTAkursus1], [@ASTAkursus2] og [@ASTAkursus3].

I statistisk inferens differentieres der mellem to metoder, _estimering_ og _hypotesetest_. Når der estimeres på baggrund af en population, bruges stikprøven til at beskrive en ukendt del af populationen. Det kan for eksempel være gennemsnitsindkomst af danskere, $\mu$, for hvilken der findes et estimat $\hat{\mu}$, som bruges til at beskrive $\mu$. Dette betegnes som et punktestimat, og vil oftest suppleres med et intervalestimat. Årsagen til dette er, at punktestimater er tilfældige, og derfor ændrer sig fra stikprøve til stikprøve. Da punktestimaters sandsynlighed for at være korrekt derfor er lig $0$, tilstræbes det at anvende et intervalestimat, hvor det kan siges at $\mu$ med $95\%$ sikkerhed ligger, fremfor et punktestimat. Dette kaldes for et konfidensinterval.

Den anden form for statistisk infernes er hypotesetest. I hypotesetest opstilles først en nulhypotese, $H_0$, og en alternativ hypotese, $H_1 = \neg H_0$. Formålet med en hypotesetest er at indsamle nok evidens imod $H_0$, så den kan forkastes, hvilket giver ny viden omkring populationen. En nulhypotese kunne være, at danskere tjener det samme i gennemsnit som folk fra Sverige. Der vil ved hjælp af forskellige statistike metoder, ses om der er en signifikant forskel i gennemsnittet. Hvis dette er tilfældet, vil nulhypotesen forkastes og den alternative hypotese vil antages at være sand.

## Hypotesetest

I det følgende afsnit introduceres hypotesetests, samt hvorledes disse anvendes til at drage konklusioner for en population, ved at opstille to hypoteser.

En hypotesetest baserer sig på det videnskabelige princip om falsificering. Der opstilles en indledende formodning om en population, kaldet nulhypotesen $H_0$, og en alternativ, modsat hypotese $H_1$. Er den indledende formodning ikke korrekt, må den alternative hypotese være gældende.

Ved en hypotesetest undersøges, hvorvidt der er en difference mellem observerede værdier og forventede værdier, hvis $H_0$ er sand.

Sandsynligheden for at der er en difference er stor, eftersom der arbejdes på en stikprøve og ikke selve populationen, og derfor benyttes et mål for, hvornår differencen er _for_ stor, kaldet signifikansniveauet, $\alpha$.

En hypotesetest viser sandsynligheden for mulige udfald, for på den måde at undersøge, hvorvidt $H_0$ kan forkastes, [@HvorforHYPO].

Et mål for, hvor usandsynlig en observeret værdi er, hvis $H_0$ er sand, kaldes for en teststørrelse.

For at kunne bestemme, om en difference mellem en observeret og forventet værdi er signifikant, benyttes en signifikanstest. En signifikanstest er en metode til at finde teststørrelsen og undersøge, om den er signifikant eller ej.

Teststørrelsen findes ofte som antallet af standardafvigelser, den observerede værdi, $\hat \theta$, ligger fra den forventede værdi $\theta_0$.

At $\hat \theta$ ligger mere end $3$ standardafvigelser fra $\theta_0$, er højst usandsynligt, da $\hat \theta$ i så fald er en outlier i populationen. I et sådan tilfælde er $\theta_0$ højst sandsynligt ikke populationens korrekte værdi.

En illustration af teststørrelsens betydning ved en normalfordeling kan ses på Figur \@ref(fig:figur-Hypotesetest).

```{r, figur-Hypotesetest, out.width='75%', fig.align='center', fig.cap = "Teststørrelsens indflydelse på nulhypotesen", echo = FALSE}
knitr::include_graphics('images/HippoHyppo.jpeg')
```

Derudover benyttes testtørrelsen til at udregne _p_-værdien, som er sandsynligheden for at få en teststørrelse, der er lige så eller mere ekstrem, hvis $H_0$ er sand.

Værdien af teststørrelsen påvirker _p_-værdien på den måde, at når teststørrelsen bliver mere ekstrem, falder _p_-værdien. Jo mindre _p_-værdien er, des mindre stoles på $H_0$, og hvis _p_-værdien er mindre end signifikansniveauet, $\alpha$, forkastes $H_0$. Er _p_-værdien derimod større end $\alpha$, er der ikke belæg for at forkaste $H_0$ - dette betyder dog ikke, at $H_0$ givetvis er sand.

Normalt arbejdes der med et signifikansniveau på $5\%$, $\alpha=0.05$. Dog er der intet fast signifikansniveau og det kunne lige såvel være $10\%$ eller $1\%$. Betydningen heraf diskuteres kort sidst i afsnittet under fejltyper, [@ASTA-HYPO].

## Hypotesetest for middelværdier {#t-test}

__FIXME__ (Afsnittet kunne bruge lidt mere forklarende teskt. Mindre punktform)

I dette afsnit gennemgås fremgangsmåden for, hvordan en hypotesetest kan bruges til at bestemme middelværdien for en population. I dette afsnit kaldes en sådan hypotesetest for en t-test. Afsnittet er skrevet på baggrund af [@ASTAkursus4].

Først er der nogle antagelser, der skal være opfyldt, for at t-testen ikke giver misvisende resultater.

1. Stikprøven er repræsentativ for populationen.
2. Variablen er kvantitativ.
3. Stikprøveudtagning er udført med tilfældighed.
4. Populationen er normalfordelt.

Herefter opstilles hypoteserne. Nulhypotesen, $H_0: \mu = \mu_0$ og den alternative hypotese $H_1: \mu \neq \mu_0$.

Dernæst sættes et signifikansniveau, $\alpha$, der vurderer med hvilken sikkerhed $H_0$ forkastes.

Derefter beregnes den observerede teststørrelse, $t_{obs} = \frac{|\bar y - \mu_o|}{\text{se}}$, hvor $\text{se} = \frac{s}{\sqrt{n}}$.

Til slut findes _p_-værdien, og på baggrund af denne, bliver nulhypotesen enten forkastet eller ej.

__Eksempel__

Der vil nu vises et eksempel på en t-test. Figur \@ref(fig:hist10) viser en stikprøve af ```r n``` observationer med en middelværdi på ```r x_middelvaerdi```, udtaget fra en standard normalfordelt population med en forventet middelværdi på $0$.

```{r include=FALSE}
set.seed(1)
n <- 10
forventet_middelvaerdi <- 0
xdata <- rnorm(n, forventet_middelvaerdi, 1)
x_middelvaerdi <- mean(xdata)
```

```{r hist10, echo=FALSE, fig.align='center', fig.cap = "Histogram over 10 simulerede standard normalfordelte tal."}
hist(xdata, main = NULL,
    ylab="Frekvens",
    xlab="Værdi")
```

I kodestykket nedenfor gennemgås den beskrevede fremgangsmåde for en t-test. I dette eksempel er $H_0: \mu = 0$, og $H_1: \mu \neq 0$.

```{r}
n <- 10
forventet_middelvaerdi <- 0 # Den forventede middelværdi er lig 0
xdata <- rnorm(n, forventet_middelvaerdi, 1) # Der udtages en tilfældig normalfordelt stikprøve
middelvaerdi <- mean(xdata) # Middelværdien af stikprøven udregnes
standardafvigelse <- sd(xdata) # Standardafvigelsen af stikprøven udregnes

estimeret_standardfejl <- standardafvigelse/sqrt(n) # Stikprøvens estimerede standardfejl udregnes

t_obs <- (abs(middelvaerdi-forventet_middelvaerdi))/estimeret_standardfejl
  # Den observerede teststørrelse udregnes

alpha_halve <-  1 - pdist("t", q = t_obs, df = n-1, plot = FALSE)
p <- 2 * alpha_halve # p-værdien udregnes
```

```{r include=FALSE}
forventet_meanx <- -0.5
t_obsx <- (abs(middelvaerdi-forventet_meanx))/
  estimeret_standardfejl
x_p = 2 * (1 - pdist("t", q = t_obsx, df = n-1, plot = FALSE))
```

Eftersom *p*-værdien er ```r p``` $> \alpha=0.05$, forkastes $H_0$ ikke. Havde det derimod været en forventet værdi på ```r forventet_meanx```, ville *p*-værdien blive ```r x_p``` $< \alpha=0.05$, hvilket vil medføre, at $H_0$ forkastes og det vil formodes, at $H_0$ ikke er korrekt for populationen.

### Fejltyper {#fejltyper-afsnit}

Der er risiko for to primære fejl når der foretages en hypotesetest, som illustreret på figur \@ref(fig:figur-typefejl). Den første, type-I fejl, er hvor $H_0$ forkastes, men i virkeligheden er sand, og den anden, type-II fejl, er hvor $H_0$ accepteres, men i realiteten er falsk.

En af de primære årsager til disse typer fejl er, hvor signifikansniveauet bliver sat, da dette i nogle tilfælde har stor betydning for, hvorvidt en hypotese bliver forkastet eller ej.

Sandsynligheden for type-I fejl er lig med det valgte signifikansniveau. Sandsynligheden for type-II fejl er derimod ikke let at præcisere. Dog er der stor sandsynlighed for type-II fejl, hvis den virkelige sandhed er tæt på nulhypotesen. Er den virkelige sandhed derimod langt fra nulhypotesen, vil sandsynligheden for type-II fejl være tilsvarende lille. Ligeledes har stikprøvens størrelse indflydelse, eftersom meget data mindsker risikoen for type-II fejl, hvor der er større risiko ved en mindre mængde data, [@Fejltyper].

```{r figur-typefejl, out.width='75%', fig.align='center', fig.cap = "Tabel over fejltyper", echo = FALSE}
knitr::include_graphics('images/Typefejl.png')
```

FIXME: Skriv om 'styrken' (jævnfør arbejdsblad fra 26/3)

### Uparret t-test for ikke-normalfordelte stikprøver {#t-test2}

I nogle tilfælde er det ikke muligt at overholde alle de pågældende antagelser, som beskrevet i afsnit \@ref(t-test), der hører til en t-test når der udføres statistisk inferens. Når dette sker, kan det ikke altid antages, at resultaterne er retvisende. I dette afsnit vil det vises, hvad der kan ske, hvis stikprøverne ikke er normalfordelte, når der arbejdes med en uparret t-test.

I dette eksempel benyttes betafordelingen, se figur \@ref(fig:figurpop1), og gammafordelingen, se figur \@ref(fig:figurpop2), til at udføre en uparret t-test.

```{r figurpop1, fig.align="center", echo=FALSE, fig.cap= "Tæthedskurve for venstreskæv betafordeling, hvor alfa = 8 og beta = 2"}
x <- seq(0, 1, length.out = 100)
y <- dbeta(x, shape1 = 8, shape2 = 2)
plot(x, y, type = "l", main = "", ylab = "", xlab = "")
```

```{r figurpop2, fig.align="center", echo=FALSE, fig.cap= "Tæthedskurve for højreskæv gammafordeling, hvor alfa = 1 og beta = 2."}
x <- seq(0, 1, length.out = 100)
y <- dgamma(x, shape = 1,  rate = 2)
plot(x, y, type = "l", main = "", ylab = "", xlab = "")
```

Nulhypotesen er $H_0: \mu_1 - \mu_2 = 0$ og den alternative hypotese er $H_1 : \mu_1 - \mu_2 \neq 0$. Denne nulhypotese undersøges ved hjælp af en uparret t-test. Der udtages en stikprøve fra hver af de viste fordelinger, som der udføres to-sidet uparret t-test på, ved hjælp af den indbyggede funktion `t.test`.

```{r, tidy=TRUE}
#t.test ud fra de to stikprøver
Stik1 <- rbeta(100, 8, 2)
Stik2 <- rgamma(100, 1, 2)
t1 <- t.test(Stik1, Stik2, alternative = "two.sided", mu = 0, conf.level = 0.95)
```

Udfra t-testen fås et konfidensinterval på [```r round(t1$conf.int, 4)```]. Forskellen mellem populationernes middelværdier vil ligge i dette interval med $95\%$ sikkerhed, ifølge t-testen. Dækningsgraden af et konfidensinterval udarbejdet fra en uparret t-test kan undersøges ved at trække nye stikprøver fra populationerne, i alt $10,000$ gange, og hver gang oprette et nyt konfidensinterval. Den sande dækningsgrad er andelen af gangene, forskellen mellem populationernes middelværdier er indeholdt i konfidensintervallerne.

```{r ronni, fig.cap= "Kode test"}
#Undersøg dækningsgraden af konfidensintervallet ved udtræk af nye stikprøver.
set.seed(31415)

middel1 <- 8/(8+2)
middel2 <- 1*(1/2)

sand_dif <- abs(middel1 - middel2)

res <- replicate(10000, {
  x1 <- rbeta(20, 8, 2)
  x2 <- rgamma(20, 1, 2)

  t_res <- t.test(x1, x2, alternative = "two.sided", conf.level = 0.95)

  konfinterval <- t_res$conf.int

  konfinterval[1L] <= sand_dif & konfinterval[2L] >= sand_dif
})

tf <- table(res)
tf
```

Det fremgår fra tabellen, at dækningsgraden af konfidensintervallerne er ```r tf[2]/100```$\%$. Dette stemmer ikke overens med antagelsen om, at konfidensintervallet har en dækningsgrad på $95\%$. Det kan derfor ikke antages, at en t-test på en ikke-normalfordelt stikprøve altid giver retvisende resultater.

## Simulering

I dette afsnit, vil der redegøres for lidt baggrundsviden omkring datasimulering, og hvordan tilsyneladende tilfældige tal kan genereres ved hjælp af algoritmer. Derefter vil der ses på hvordan programmeringssproget R, kan udnyttes til at udføre disse simuleringer hurtigt og simpelt.

Simuleringer er en generering af estimater på mulige udfald, og på den måde en efterligning af virkeligheden. Formålet er derved at generere tilfældigt estimerede værdier ud fra en model, der simulerer virkeligheden, hvilket muliggør yderligere analyser.

En definition på simulering er;

>A situation or event that seems real but is not real, used especially in order to help people deal with such situations or events. - Cambridge Dictonary, [@CambridgeSimulation].

Ud fra definitionen, er formålet altså ved simuleringer at efterligne virkeligheden, så de analyser der gøres på baggrund af simuleringerne, kan bruges i virkeligheden når lignende situationer opstår.
Brancher hvor simuleringer er et yderst vigtigt redskab, er i motorsporten. I Formel 1 benytter holdene sig af simulatorer, hvor de genskaber bilerne og derved kan teste nye dele inden de producerer dem i virkeligheden for at spare ressourcer, [@MercedesF1].

### Pseudorandom number generator

I dette afsnit introduceres pseudo-tilfældige tal, PRNG (fra engelsk _pseudorandom number generator_), og hvorledes disse kan bringes i anvendelse i forbindelse med simuleringer. I de tilhørende underafsnit beskrives først en type af PRNG kaldet lineær kongruens generator, og dernæst en metode til at omdanne uniformt fordelte talt til standard normalfordelte tal kaldet Box-Muller-transformation.

En computer fungerer ved, at den modtager et input, der bearbejdes af en algoritme, som derefter returnerer et output. Der findes ingen algoritmer, som er i stand til at generere faktisk tilfældige tal. Grunden til dette er, hvis der gives det samme input, til den samme algoritme, vil resultatet være det samme output som tidligere, fordi en computer fungerer på baggrund af matematik og derfor er deterministisk. Det er dog muligt ved hjælp af en beregningsmodel at skabe en illusion af ægte tilfældighed. En sådan model kaldes for en PRNG.

En PRNG beskrives ved nedenstående karakteristika, [@PRGNfunc].

1. Et input, eller _seed_, på baggrund af hvilket algoritmen beregner et pseudo-tilfældigt output. Herefter fortsætter algoritmen rekursivt, hvor det forrige output anvendes som nyt input.

2. cyklussen, som beskriver den talserie, PRNG'en beregner.

3. Perioden, som beskriver hvor mange repetitioner algoritmen gennemløber, før cyklussen gentages. Jo kortere perioden er, des mere gennemskuelig vil algoritmen fremstå.

4. Fordelingen af cyklussen. Som det ses på figur \@ref(fig:PRNG-fordelinger), kan fordelingen af cyklussen være jævn, hvilket viser en ligelig fordeling af cyklussen. Ses der derimod et mønster eller tendens i fordelingen, vil algoritmen ikke være forventningsret, og anvendeligheden formindskes.

```{r PRNG-fordelinger, out.width='75%', fig.align='center', fig.cap="PRNG fordelinger", echo = FALSE}
 knitr::include_graphics("images/prngfordelingerDK.jpeg")
```

En af de tidligste algoritmer til PRNG, _middle-square method_, blev udvilket af John von Neumann. Svagheden ved denne metode er, at dens periode oftest er ret kort og derfor begynder cyklussen hurtigt at gentage sig, [@PRNGintro, s. 12-13]. I takt med udviklingen af de teknologiske hjælpemidler er der opstået mere effektive algoritmer til PRNG. En af de mere kendte og hyppigt anvendte algoritmer er lineær kongruens generator, som gennemgåes i det følgnede afsnit.

### Lineær kongruens generator

Dette afsnit beskriver, hvorledes lineær kongruens generatorer, LCG (fra engelsk _linear congruential generator_), på baggrund af en arbitrære parametre, kan generere en cyklus af tilsyneladende tilfældige tal, som er uniformt fordelt, ved valg af passende parametre. Det følgende afsnit er primært baseret på kilden [@LCGinfo].

LCG, der er en af de mange PRNG-generatorer, danner en cyklus af tal ved iteration, og kræver kun få parametre. Helt specifikt er algoritmen givet ved

>```{definition}
>Givet $a, c, m \in \mathbb{Z}$, hvor $0 < m, 0 < a$ og $0 \leq c \leq m$, genererer algoritmen lineær kongruens generator en talsekvens, $X = [X_0, X_1, \ldots, X_n]$, givet ved $X_i = (a \cdot X_{i-1} + c) \mod m.$
>```

Algoritmen fungerer således:
Der startes ved et _seed_ angivet med $X_0$, som bliver multipliceret med faktoren, $a$, og konstantleddet, $c$, adderes. Derefter tages modulus $m$ af værdien.

I figuren nedenfor kan ses tre eksempler på LCG. De to første eksempler har samme parametre, men forskelligt _seed_.

```{r, out.width='75%', fig.align='center', fig.cap=" 3 eksempler på LCG. [@lincongenvis]", echo = FALSE}
 knitr::include_graphics("images/LCG_eksempel.PNG")
```

I teorien er parametrene arbitrære, men i praksis bruges nogle værdier oftere end andre. Begrundelsen for dette er, at der findes "dårlige" værdier for parametrene, der vil returnere en åbentlyst ikke-tilfældig cyklus. Et eksempel på dette er ved parametrene: $m = 64, a = 33$ og $c = 12$. Efter et antal iterationer vil der ses et mønster i værdierne, og det blotte øje vil kunne se, at denne cyklus i virkeligheden ikke er tilfældig. Dette eftervises i det følgende eksempel.

__Eksempel__

De følgende simuleringer er baseret på kilden [@LCGsimu], som også går mere i dybden med hvordan, der undersøges om de givne parametre genererer tilsyneladende tilfældige cyklusser.

Først defineres en funktion for LCG-algoritmen, som tager de nødvendige parametre for algoritmen som inputs. Derudover angives også et argument, $n$, der fortæller hvor mange iterationer algoritmen skal foretage.

```{r}
lcg <- function(m, a, c, seed, n) {
  #M er modulus, a er faktoren, c er tilvæksten, seed er
  #startværdien og n er antal iterationer.

  r <- c()
  r[1] <- seed #Angiver startpunktet.

  for (i in 1: (n-1)) {
  r[i + 1] <- (a * r[i] + c) %% m

}
 return(r)
}
```

Det følgende er et eksempel på en LCG-cyklus med en periode på $16$.

```{r}
lcg1 <- lcg(64, 33, 12, 57, 17)
lcg1
```

Forneden i figur \@ref(fig:ikke-tilfaeldig-LCG) plottes denne cyklus. Det er åbenlyst at cyklussen ikke er tilfældig, da den danner et synligt mønster. Selv hvis antallet af gange, LCG køres igennem, ændres, vil mønstret ikke ændres.

```{r ikke-tilfaeldig-LCG, fig.cap= "Et eksempel på en ikke-tilfældig LCG-cyklus."}
lcg1_plot <- lcg(64, 33, 12, 57, 100)
plot(lcg1_plot, main = "",
     xlab = "Indeks", ylab = "Værdier", type = "l")
```

Nedenfor i figur \@ref(fig:uniform-LCG) vises et eksempel på, hvordan LCG-algoritmen kan bruges til at lave en uniform fordeling i intervallet [0, 1].

```{r uniform-LCG, fig.cap= "Et eksempel på en LCG-cyklus, der er uniformt fordelt."}
lcg2 <- lcg(86436, 1093, 0, 12, 1000)

hist((lcg2 + 0.05)/ 86436, main = "",
     xlab = "Værdier", ylab = "Frekvens")
```

Her kan det også nævnes, at hvis den cyklus algoritmen returnerer plottes i et 2d-punktplot, som i figur \@ref(fig:plot-LCG-cyklus), vil der fremgå en tydeligere "tilfældig" fordeling, dog vil der stadig kunne ses et mønster.

```{r plot-LCG-cyklus, fig.cap= "En 2d-graf af en 'tilfældig' LCG-cyklus."}
plot(lcg2, main = "", xlab = "Indeks",
     ylab = "Værdier")
```

Afslutningsvist vil der vises et eksempel, hvor der returneres en cyklus, der ser tilfældig ud. Denne cyklus kan ses i figur \@ref(fig:tilfaeldig-LCG).

```{r tilfaeldig-LCG, fig.cap= "Et eksempel på en tilsyneladende tilfældig LCG-cyklus."}
lcg3 <- lcg(86436, 1093, 18257, 12, 100)

plot(lcg3, main = "", xlab = "Indeks",
     ylab = "Værdier", type = "l")
```

### Box-Muller-transformation

I dette afsnit beskrives Box-Muller-transformation, hvilket er en metode til at generere standard normalfordelte tal ud fra uniformt fordelte tal. Dette gøres for at belyse, hvordan en computer kan generere tilsyneladende tilfældige tal, der er normalfordelte. Afsnittet er skrevet med inspiration fra [@Box-Muller]. Metoden beskrives konkret i nedenstående sætning.

>```{theorem}
> Box-Muller-transformation
>
> Antag, at $U_1$ og $U_2$ er uafhængige stokastiske variabler, der begge er uniformt fordelt på intervallet $[0, 1]$. Lad $$Z_1 = \sqrt{-2 \mathrm{ln} U_1} \mathrm{cos}(2\pi U_2)~ \wedge~ Z_2 = \sqrt{-2 \mathrm{ln} U_1} \mathrm{sin}(2\pi U_2).$$ Så er $Z_1$ og $Z_2$ uafhængige stokastiske variabler, der er standard normalfordelte.
>```

__Eksempel__

I dette afsnit oprettes to normalfordelinger ved hjælp af Box-Muller-transformationen i R.

Først simuleres to stokatiske variabler, ```U1``` og ```U2```, hvor $\mathrm{U1 \sim unif(0,1)}$ og $\mathrm{U2 \sim unif(0,1)}$.

```{r, echo = FALSE, message=FALSE, include=FALSE}
set.seed(2000)
library(mosaic)
library(e1071)
```

```{r}
U1 <- runif(n = 100000, min = 0, max = 1)
U2 <- runif(n = 100000, min = 0, max = 1)
```

Disse to stokatiske variabler benyttes nu til at oprette de to påstået standard normalfordelte stokatiske variabler, ```Z1``` og ```Z2```.

```{r}
Z1 <- sqrt(-2*log(U1))*cos(2*pi*U2)
Z2 <- sqrt(-2*log(U1))*sin(2*pi*U2)
```

__Efterprøvning i R__

I det følgende afsnit efterprøves de forventede karakteristika af de to stokastiske variabler $Z_1$ og $Z_2$, der blev oprettet ved hjælp af Box-Muller-transformationen.

Først undersøges deskriptivt, hvilken fordeling ```Z1``` og ```Z2``` har, ved at oprette et boksplot af de to stokastiske variabler, som ses i figur \@ref(fig:boksplotz1z2).

```{r boksplotz1z2, fig.cap= "Et boksplot over Z1 og Z2"}
boxplot(Z1, Z2, main = "", xlab= c("Z1","Z2"))
```

Det ses på boksplottet, at der er en indikation på normalfordeling med henvisning til de fire krav et boksplot af en normalfordeling opfylder. De fire krav er: __FIXME__ (Kilde?)

1) Median og middelværdi er lig hinanden.

2) Øvre og nedre kvartil er lige langt fra midten.

3) Der er lige mange outliers over øvre kvartil som under nedre kvartil.

4) $0.7\%$ af observationerne ligger som outliers.

Punkterne 2 og 3 tjekkes ved at betragte boksplottet, hvor der ikke umiddelbart synes at være noget, der modbeviser en normalfordeling af ```Z1``` og ```Z2```.

For at tjekke punkt 1, beregnes median (```median```) og middelværdi (```mean```) i kodestykket nedenfor.

```{r}
mean_Z1 <- mean(Z1)
median_Z1 <- median(Z1)

mean_Z2 <- mean(Z2)
median_Z2 <- median(Z2)


```

Dette giver henholdsvis en middelværdi og median for ```Z1``` på $`r mean_Z1`$ og $`r median_Z1`$, samt for ```Z2```  på $`r mean_Z2`$ og $`r median_Z2`$. Disse resultater ligger meget tæt på hinanden, og den meget lave værdi stemmer overens med forventningen om, at ```Z1``` og ```Z2``` er standard normalfordelte.

For at tjekke punkt $4$, beregnes andelen af outliers i hvert boksplot i kodestykket nedenfor.

```{r}
OutVals1 <- boxplot(Z1, plot = FALSE)$out
outliers_Z1 <- length(OutVals1)

OutVals2 <- boxplot(Z2, plot = FALSE)$out
outliers_Z2 <- length(OutVals2)

outliers_andel_Z1 <- outliers_Z1/length(Z1)
outliers_andel_Z2 <- outliers_Z2/length(Z2)
```

Dette giver en andel af outliers i ```Z1``` på ```r outliers_andel_Z1``` og i ```Z2``` på ```r outliers_andel_Z2```. Dette svarer til cirka $0,7~ \%$ outliers i både ```Z1``` og ```Z2```, hvilket stemmer overens med punkt 4.

Der er altså ikke noget evidens imod, at ```Z1``` og ```Z2``` skulle være normalfordelt. Tværtimod underbygger beregningen af deres middelværdi, at de er _standard_ normalfordelte.

Hernest kigges der på histogrammerne for ```Z1``` og ```Z2```. På figur \@ref(fig:bmhist) sammenlignes histogrammerne for henholdsvis ```Z1``` og ```Z2``` med densiteten for standard normalfordeling.

__FIXME__ (Lav plot om til figur, figuren skal have ref. i tekseten, samt forklaring i brødtekst.) __FIXME__ (Sammenlign med Z-fordelings densitet)
```{r bmhist, fig.cap= "Sammenligning af histogrammerne for henholdsvis Z1 (a) og Z2 (b) med densiteten for standard normalfordeling."}
x <- seq(-4, 4, length.out = 100)
y <- dnorm(x, mean = 0, sd = 1)

par(mfrow = c(1,2))

hist(Z1, main = "(a)", ylab = "Frekvens", prob = TRUE)
lines(x, y, col = "blue")

hist(Z2, main = "(b)", ylab = "Frekvens", prob = TRUE)
lines(x, y, col = "blue")

```

Det ses på histogrammerne, at de tilnærmelsesvist er normalfordelte, samt at de har en middelværdi på $\approx 0$ og en standardafvigelse på $\approx 1$, og derved er  ```Z1``` og ```Z2``` standard normalfordelte.

Desuden er det også en konsekvens af Box-Muller-transformationerne, at ```Z1``` og ```Z2``` er uafhængige. Dette undersøges ved hjælp af nedenstående graf.

__FIXME:__ (Husk at indsætte figuren inden aflevering! :-))
```{r}
plot(Z2 ~ Z1, main = "Graf for uafhængighed mellem Z1 og Z2",
    col = rgb(red = 0, green = 0, blue = 0, alpha = 0.25),
    pch = 1, cex = 0.5)

```

Udover at eftervise uafhængigheden af ```Z1``` og ```Z2``` grafisk, kan det også undersøges ved hjælp af funktionen ```cor``` i R, som giver en værdi for korrelationen af de to talsekvenser.
Korrelationen af to variabler udregnes ved formlen

$$
cor(X, Y) = \frac{cov(X, Y)}{sd(X) \cdot sd(Y)}
$$
Hvor $cov$ angiver kovariansen og $sd$ angiver standardafvigelsen.

Korrelation fortæller, hvor stor en lineær sammenhæng der er mellem to variabler. Denne værdi ligger i intervallet $[-1, 1]$, hvor $-1$ påviser en perfekt negativ lineær sammenhæng, $1$ påviser en perfekt positiv lineær sammenhæng, og $0$ viser, at der ingen lineær sammenhæng er.

```{r}
cor(Z1, Z2)
```

Et resultat på afrundet $-0.00048$, hvilket næsten er 0, viser altså også at der ingen lineær korrelation er mellem ```Z1``` og ```Z2```, hvilket giver en indiktion for, at de er uafhængige.

### Simulering i R {#pop-sim}

I dette afsnit gives eksempler på, hvorledes der kan udføres simuleringer af forskellige fordelinger, således disse fordelinger kan anvendes til statistisk inferens. Afsnittet er primært baseret på kilden [@Rsimulation].

<!-- Simuleringer i R udføres ved at estimere udfald fra en fordeling, hvor der bliver generet pseudo-tilfældige tal. Disse generede tal forekommer at være tilfældige, men er det reelt set ikke, [@PRNG]. Tallene er genereret ud fra et _seed_, som i R bestemmes ved ```set.seed("værdi")```. Dette _seed_ vil nu angives, så de følgende genereret tal vil være ens, ved nye gennemkørsler. -->

I dataanalysen vil de viste fordelinger bruges som populationer, og stikprøver udtages så ved hjælp af forskellige funktioner. Idéen med denne fremgangsmåde er, at der derved udtages stikprøver fra teoretisk uendelige fordelinger.

Nogle af de fordelinger, der kan simuleres i R, er normal-, binomial-, uniforme-, beta- og gammafordelinger. Alle disse fordelinger har forskellige karateristika, som påvirker hvilke værdier de kan antage. Der vil nu vises eksempler på, hvordan disse fordelinger simuleres i R, og hvordan resultatet vil se ud.

#### Normalfordeling

I R simuleres en normalfordeling ved funktionen ```rnorm```. Fordelingen på figur \@ref(fig:normal) bliver genereret ud fra en middelværdi på $0$ og en standardafvigelse på $1$.


```{r normal, echo = FALSE, fig.cap = "En normalfordeling med størrelsen n = 1000, en middelværdi på 0 og en standardafvigelse på 1. Med disse værdier kaldes den også for en standard normalfordeling eller Z-fordeling"}
set.seed(29)
x <- seq(-4, 4, length.out = 100)
y <- dnorm(x, mean = 0, sd = 1)
plot(x, y, type = "l", main = "", ylab = "Densitet", xlab = "x")
```

#### Uniform fordeling

En uniform fordeling er defineret ud fra et interval, som der skal genereres et givent antal værdier indenfor. Alle disse værdier har lige stor sandsynlighed for at blive genereret. Jo større $n$ er, des mere uniformt bliver histogrammet.

```{r}
uniformfordeling <- runif(n = 100000, min = 1, max = 2)
```

__FIXME__ (ref og forklaring af figuren i teksten)

```{r echo = FALSE, fig.cap = "En uniform fordeling, hvor alle tal har lige stor sandsynlighed for at optræde i intervallet [0 , 1]."}
x <- seq(1, 2, length.out = 100)
plot(x = NA, type = "n", xlim = c(0, 1), ylim = c(0, 1.5), ylab = "Densitet", xlab = "x")
abline(h = 1)
```

#### Binomialfordeling

__FIXME__ (udbyd beskrivelsen af fordelingen)

En binomialfordeling bliver genereret ud fra sandsynligheder. I den binomiale fordeling gives en _size_ som er antallet af eksperimenter, samt en sandsynlighed for succes, angivet med _prob_.

```{r}
Binomialfordeling <- rbinom(n = 100, size = 1, prob = 0.5)
```

__FIXME__ (ref og forklaring af figuren i teksten)

```{r echo = FALSE, fig.cap = "En binomialfordeling med size = 1 og en sandsynlighed for succes på prob = 0.5."}
plot(x = NA, xlim = c(0,1), ylim = c(0, 0.6), main = "", ylab = "Densitet", xlab = "x")
segments(x0 = 0, y0 = 0, x1 = 0, y1 = 0.5)
segments(x0 = 1, y0 = 0, x1 = 1, y1 = 0.5)
segments(x0 = 0, y0 = 0, x1 = 1, y1 = 0)
```


En egenskab ved binomialfordelinger er, at jo højere $n$ er, des tættere vil middelværdien af stikprøven være på sandsynligheden for succes. I dette tilfælde med en størrelse på $n = 100$, er middelværdien ```r mean(Binomialfordeling)```.

#### Skæve fordelinger

En skæv fordeling er kendetegnet ved, at størstedelen af observationerne er samlet omkring visse værdier, mens de resterende observationer fordeler sig ud til kun én af siderne. De resterende observationer kaldes for "halen" af fordelingen, og alt efter retningen af dem, kaldes den skæve fordeling for "venstreskæv" elller "højreskæv".

##### Betafordeling

En skæv fordeling kan være en betafordeling, $\text{Beta}(\alpha, \beta)$, hvor $\alpha-1$ angiver andelen af succeser og $\beta-1$ angiver andelen af fiaskoer. Betafordelingen er tilnærmelsesvis normaltfordelt, hvis $\alpha$ og $\beta$ begge er tilpas store og omtrent ens, [@TDSBeta].

En venstreskæv og en højreskæv betafordeling vil nu simuleres, hvor $\alpha = 8$ og $\beta = 2$ for den venstreskæve, mens $\alpha = 2$ og $\beta = 8$ for den højreskæve.

```{r}
venstre <- rbeta(n = 1000, shape1 = 8, shape2 = 2)
hoejre <- rbeta(n = 1000, shape1 = 2, shape2 = 8)
```

__FIXME__ (ref og forklaring af figuren i teksten)

```{r echo = FALSE, fig.cap = "To betafordelinger med henholdsvis alfa = 8 og beta = 2, og alfa = 2 og beta = 8."}
x <- seq(0, 1, length.out = 100)
y1 <- dbeta(x, shape1 = 2, shape2 = 8)
y2 <- dbeta(x, shape1 = 8, shape2 = 2)

plot(x, y1, type = "l", main = "", ylab = "Densitet", xlab = "x", col = "blue")
lines(x, y2, col = "red")
legend(0.35, 3, legend=c("alfa = 2, beta = 8 ", "alfa = 8, beta = 2"),
       col=c("blue", "red"), lty=c(1,1,1), cex=0.8)
```


##### Gammafordeling

Gammafordelingen er ligeledes en skæv fordeling. Funktionaliteten af gammafordelingen er at forudsige ventetiden til den $\alpha$'te begivenhed. Tæthedsfunktionen for gammafordelingen, er beskrevet med to forskellige parametersæt, henholdsvis $(k,~\theta)$ og $(\alpha,~\beta)$. I denne rapport vil der fremefter blive anvendt  parameterne $(\alpha,~\beta)$. Tæthedsfuktionen er givet ved $f(x)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$, hvor $\alpha > 0$ er "shape", $\beta > 0$ er "rate" og $x \in ( 0, \infty)$, [@Gammadist]. På figur \@ref(fig:Gammafordeling) er vist tre forskellige højreskæve gammafordelinger.

```{r}
Gammafordeling <- rgamma(n = 100000, shape = 5, rate = 1)
```


```{r Gammafordeling, echo = FALSE, fig.cap = "Tre gammafordelinger hvor beta = 1 og alfa på henholdsvis 1, 5 og 10."}
x <- seq(0, 25, length.out = 100)
y1 <- dgamma(x, shape = 1, rate = 1)
y2 <- dgamma(x, shape = 5, rate = 1)
y3 <- dgamma(x, shape = 10, rate = 1)
plot(x, y1, type = "l", main = "", ylab = "Densitet", xlab = "x", col = "blue")
lines(x, y2, col = "red")
lines(x, y3, col = "green")
legend(20, 0.8, legend=c("alfa = 1", "alfa = 5", "alfa = 10"),
       col=c("blue", "red", "green"), lty=c(1,1,1), cex=0.8)
```

### Replicate

I nogle statistiske metoder er det nødvendigt at arbejde med flere stikprøver end kun én enkelt. Med funktionen ```replicate``` i R, kan der genereres adskillige nye stikprøver ud fra den oprindelige. Processen med _replicate_ vil nu vises. Først ses en stikprøve med tre observationer.

```{r echo = FALSE}
stik <- rnorm(3, 0 ,1) #Den oprindelige stikprøve
stik_matrix <- matrix(stik, nrow = 3)
colnames(stik_matrix) <- c("Oprindelig stikprøve")
rownames(stik_matrix) <- c("Observation 1", "Observation 2", "Observation 3")
stik_matrix
```

_Replicate_-funktionen vil nu bruges. I alt replikeres der fem gange, hvilket betyder, at der bliver oprettet fem nye stikprøver. I forbindelse med denne process anvendes funktionen ```sample```, som er funktionen der _resampler_, enten med eller uden tilbagelægning fra stikprøven. I dette eksempel bruges funktionen med tilbagelægning. Fra den oprindelige stikprøve udtrækkes en observation, som indsættes i en ny stikprøve. Observationeren bliver derefter lagt tilbage i den oprindelige stikprøve, dette medfører at den samme observation kan udtages flere gange. Grunden til dette er, at stikprøven anses for at være repræsentativ for population, og to observationer af den samme værdi derfor ikke er utænkelig.

```{r}
ny_stik <- replicate(5, { #Der replikeres 5 gange
  sample(stik, size = 3, replace = TRUE)
})
ny_stik
```

Funktionen har nu resamplet fem nye repræsentative stikprøver udfra den oprindelige. Denne metode og dens anvendelser vil der kigges nærmere på kapitel \@ref(boot-afsnit).

## Problemformulering

_Kan simuleringsstudier bidrage til at højne kvaliteten af dataanalyser, og i så fald, hvordan?_

1. Hvilke faldgruber skal man være opmærksom på ved hypotesetest?

2. Hvordan simulerer en computer data?

3. Hvordan kan teoretiske resultater eftervises ved hjælp af simulering?

__FIXME:__ arbejdsspørgsmål i samme rækkefølge som i rapporten. Ret løbende.
