```{r, include=FALSE}
library(mosaic)
set.seed(29)
```


# Simulering

I dette afsnit, vil der redegøres for lidt baggrundsviden omkring simulering, og hvordan tilsyneladende tilfældige tal kan genereres ved hjælp af algoritmer. Derefter vil der ses på, hvordan programmeringssproget R kan udnyttes, til at udføre disse simuleringer hurtigt og simpelt.

En definition på simulering er;

>A situation or event that seems real but is not real, used especially in order to help people deal with such situations or events. - Cambridge Dictonary, [@CambridgeSimulation].

Ud fra definitionen, er formålet altså ved simuleringer at efterligne virkeligheden, så de analyser der gøres på baggrund af simuleringerne, kan bruges i virkeligheden når lignende situationer opstår.

Brancher hvor simuleringer er et yderst vigtigt redskab, er i motorsporten. I Formel 1 benytter holdene sig af simulatorer, hvor de genskaber bilerne og derved kan teste nye dele inden de producerer dem i virkeligheden for at spare ressourcer, [@MercedesF1].

## Pseudorandom number generator

I dette afsnit introduceres pseudo-tilfældige talgeneratorer, PRNG'er (fra engelsk _pseudorandom number generators_), og hvorledes disse kan bringes i anvendelse i forbindelse med simuleringer. I de tilhørende underafsnit beskrives først en type af PRNG kaldet lineær kongruens generator, og dernæst en metode til at omdanne uniformt fordelte talt til standard normalfordelte tal kaldet Box-Muller-transformation.

En computer fungerer ved, at den modtager et input, der bearbejdes af en algoritme, som derefter returnerer et output. Der findes ingen algoritmer, som er i stand til at generere faktisk tilfældige tal. Grunden til dette er, hvis der gives det samme input, til den samme algoritme, vil resultatet være det samme output som tidligere, fordi en computer fungerer på baggrund af matematik og derfor er deterministisk. Det er dog muligt ved hjælp af en beregningsmodel at skabe en illusion af ægte tilfældighed. En sådan model kaldes for en PRNG.

En PRNG beskrives ved nedenstående karakteristika, [@PRGNfunc].

1. Et input, eller _seed_, på baggrund af hvilket algoritmen beregner et pseudo-tilfældigt output. Herefter fortsætter algoritmen rekursivt, hvor det forrige output anvendes som nyt input.

2. Cyklussen, som beskriver den talserie, PRNG'en beregner.

3. Perioden, som beskriver hvor mange repetitioner algoritmen gennemløber, før cyklussen gentages. Jo kortere perioden er, des mere gennemskuelig vil algoritmen fremstå.

4. Fordelingen af cyklussen. Som det ses på figur \@ref(fig:PRNG-fordelinger), kan fordelingen af cyklussen være jævn, hvilket viser en ligelig fordeling af cyklussen. Ses der derimod et mønster eller tendens i fordelingen, vil algoritmen ikke være forventningsret, og anvendeligheden formindskes.

```{r PRNG-fordelinger, out.width='75%', fig.align='center', fig.cap="PRNG fordelinger", echo = FALSE}
 knitr::include_graphics("images/prngfordelingerDK.jpeg")
```

I takt med udviklingen af de teknologiske hjælpemidler er der opstået mere effektive algoritmer til PRNG. En af de mere kendte og hyppigt anvendte algoritmer er lineær kongruens generator, som gennemgåes i det følgnede afsnit.

### Lineær kongruens generator

Dette afsnit beskriver, hvorledes lineær kongruens generatorer, LCG (fra engelsk _linear congruential generator_), kan generere en cyklus af tilsyneladende tilfældige tal, som er uniformt fordelt, ved valg af passende parametre. Det følgende afsnit er primært baseret på kilden [@LCGinfo].

LCG danner en cyklus af tal ved iteration, og kræver kun få parametre. Helt specifikt er algoritmen givet ved

>```{definition}
>Givet $a, c, m \in \mathbb{Z}$, hvor $0 < m, 0 < a$ og $0 \leq c \leq m$, genererer algoritmen lineær kongruens generator en talsekvens, $X = [X_0, X_1, \ldots, X_n]$, givet ved $X_i = (a \cdot X_{i-1} + c) \mod m.$
>```

Algoritmen fungerer således:
Der startes ved et _seed_, angivet med $X_0$, som bliver multipliceret med faktoren, $a$, og konstantleddet, $c$, adderes. Derefter tages modulus $m$ af værdien.

I figuren nedenfor kan ses tre eksempler på LCG. De to første eksempler har samme parametre, men forskelligt _seed_.

```{r, out.width='75%', fig.align='center', fig.cap="Tre eksempler på algoritmen lineær kongruens generator. De to første eksempler har samme parametre, men forskelligt seed. [@lincongenvis]", echo = FALSE}
 knitr::include_graphics("images/LCG_eksempel.PNG")
```

I teorien er parametrene arbitrære, men i praksis bruges nogle værdier oftere end andre. Begrundelsen for dette er, at der findes værdier for parametrene, der vil returnere en åbentlyst ikke-tilfældig cyklus. Et eksempel på dette er ved parametrene $m = 64, a = 33$ og $c = 12$. Efter et antal iterationer vil der ses et mønster i værdierne, og det vil være åbenlyst, at denne cyklus i virkeligheden ikke er tilfældig. Dette eftervises i det følgende eksempel.

__Eksempel__

De følgende simuleringer er baseret på kilden [@LCGsimu], som også går mere i dybden med hvordan, der undersøges om de givne parametre genererer tilsyneladende tilfældige cyklusser.

Først defineres en funktion for LCG-algoritmen, som tager de nødvendige parametre for algoritmen som inputs. Derudover angives også et argument, $n$, der fortæller hvor mange iterationer algoritmen skal foretage.

```{r}
lcg <- function(m, a, c, seed, n) {
  #M er modulus, a er faktoren, c er tilvæksten, seed er
  #startværdien og n er antal iterationer.

  r <- c()
  r[1] <- seed #Angiver startpunktet.

  for (i in 1: (n-1)) {
  r[i + 1] <- (a * r[i] + c) %% m

}
 return(r)
}
```

Det følgende er et eksempel på en LCG-cyklus med en periode på $16$.

```{r}
lcg1 <- lcg(64, 33, 12, 57, 17)
lcg1
```

På figur \@ref(fig:ikke-tilfaeldig-LCG) plottes denne cyklus. Det er åbenlyst at cyklussen ikke er tilfældig, da den danner et synligt mønster. Selv, hvis antallet af gange, LCG køres igennem, ændres, vil mønstret ikke ændres.

```{r ikke-tilfaeldig-LCG, fig.align='center', fig.cap= "Et eksempel på en ikke-tilfældig LCG-cyklus."}
lcg1_plot <- lcg(64, 33, 12, 57, 100)
plot(lcg1_plot, main = "",
     xlab = "Indeks", ylab = "Værdier", type = "l")
```

På figur \@ref(fig:uniform-LCG) vises et eksempel på, hvordan LCG-algoritmen kan bruges til at lave en uniform fordeling i intervallet [0, 1].

```{r uniform-LCG, fig.align='center', fig.cap= "Et eksempel på en LCG-cyklus, der er uniformt fordelt."}
lcg2 <- lcg(86436, 1093, 0, 12, 1000)

hist((lcg2 + 0.05)/ 86436, main = "",
     xlab = "Værdier", ylab = "Frekvens")
```

Her kan det også nævnes, at hvis den cyklus, algoritmen returnerer, plottes i et 2d-punktplot, som på figur \@ref(fig:plot-LCG-cyklus), vil der fremgå en mere tilfældig fordeling, dog vil der stadig kunne ses et mønster.

```{r plot-LCG-cyklus, fig.align='center', fig.cap= "En 2d-graf af en LCG-cyklus, hvor der ses et svagt mønster."}
plot(lcg2, main = "", xlab = "Indeks",
     ylab = "Værdier")
```

Afslutningsvist vil der vises et eksempel, hvor der returneres en cyklus, der ser tilfældig ud. Denne cyklus kan ses på figur \@ref(fig:tilfaeldig-LCG).

```{r tilfaeldig-LCG, fig.align='center', fig.cap= "Et eksempel på en tilsyneladende tilfældig LCG-cyklus."}
lcg3 <- lcg(86436, 1093, 18257, 12, 100)

plot(lcg3, main = "", xlab = "Indeks",
     ylab = "Værdier", type = "l")
```

### Box-Muller-transformation

I dette afsnit beskrives Box-Muller-transformation, hvilket er en metode til at generere standard normalfordelte tal ud fra uniformt fordelte tal. Dette gøres for at belyse, hvordan en computer kan generere tilsyneladende tilfældige tal, der er normalfordelte. Afsnittet er skrevet med inspiration fra [@Box-Muller]. Metoden beskrives konkret i nedenstående sætning.

>```{theorem}
> Box-Muller-transformation
>
> Antag, at $U_1$ og $U_2$ er uafhængige stokastiske variabler, der begge er uniformt fordelt på intervallet $[0, 1]$. Lad $$Z_1 = \sqrt{-2 \mathrm{ln} U_1} \mathrm{cos}(2\pi U_2)~ \wedge~ Z_2 = \sqrt{-2 \mathrm{ln} U_1} \mathrm{sin}(2\pi U_2).$$ Så er $Z_1$ og $Z_2$ uafhængige stokastiske variabler, der er standard normalfordelte.
>```

__Eksempel__

I dette afsnit oprettes to normalfordelinger ved hjælp af Box-Muller-transformationen i R.

Først simuleres to stokatiske variabler, ```U1``` og ```U2```, hvor $\mathrm{U1 \sim unif(0,1)}$ og $\mathrm{U2 \sim unif(0,1)}$.

```{r}
U1 <- runif(n = 100000, min = 0, max = 1)
U2 <- runif(n = 100000, min = 0, max = 1)
```

Disse to stokatiske variabler benyttes nu til at oprette de to påstået standard normalfordelte stokatiske variabler, ```Z1``` og ```Z2```.

```{r}
Z1 <- sqrt(-2*log(U1))*cos(2*pi*U2)
Z2 <- sqrt(-2*log(U1))*sin(2*pi*U2)
```

__Efterprøvning i R__

I det følgende afsnit efterprøves de forventede karakteristika af de to stokastiske variabler $Z_1$ og $Z_2$, der blev oprettet ved hjælp af Box-Muller-transformationen.

Først undersøges deskriptivt, hvilken fordeling ```Z1``` og ```Z2``` har, ved at oprette et boksplot af de to stokastiske variabler, som ses i figur \@ref(fig:boksplotz1z2).

```{r boksplotz1z2, echo = FALSE, fig.align='center', fig.cap= "Et boksplot over Z1 og Z2"}
par(mfrow = c(1,2))
boxplot(Z1, xlab = "Z1", main = "")
boxplot(Z2, xlab = "Z2", main = "")
```

Det ses på boksplottet, at der er en indikation på normalfordeling med henvisning til de fire krav et boksplot af en normalfordeling opfylder. De fire krav er:

1) Idet en normalfordeling er symmetrisk fordelt omkring dens middelværdi, er medianen og middelværdien lig hinanden.

2) Grundet normalfordelingens symmetri, er øvre og nedre kvartil lige langt fra medianen.

3) På grund af symmetrien i en normalfordeling, er der lige mange outliers over øvre kvartil som under nedre kvartil.

4) Da en outlier i et boksplot identificeres som data, der ligger mere end $1.5 \cdot \text{IQR}$ udover nedre og øvre kvartil, medfører det, at data, der ligger mere end $2.68$ standardafvigelser fra middelværdien er outliers. Dermed følger, at $0.7\%$ af observationerne ligger som outliers.

Punkterne 2 og 3 tjekkes ved at betragte boksplottet, hvor der ikke umiddelbart synes at være noget, der modbeviser en normalfordeling af ```Z1``` og ```Z2```.

For at tjekke punkt 1, beregnes median (```median```) og middelværdi (```mean```) i kodestykket nedenfor.

```{r}
mean_Z1 <- mean(Z1)
median_Z1 <- median(Z1)

mean_Z2 <- mean(Z2)
median_Z2 <- median(Z2)
```

Dette giver henholdsvis en middelværdi og median for ```Z1``` på $`r mean_Z1`$ og $`r median_Z1`$, samt for ```Z2```  på $`r mean_Z2`$ og $`r median_Z2`$. De meget lave værdi stemmer overens med forventningen om, at ```Z1``` og ```Z2``` er standard normalfordelte.

For at tjekke punkt $4$, beregnes andelen af outliers i hvert boksplot i kodestykket nedenfor.

```{r}
OutVals1 <- boxplot(Z1, plot = FALSE)$out
outliers_Z1 <- length(OutVals1)

OutVals2 <- boxplot(Z2, plot = FALSE)$out
outliers_Z2 <- length(OutVals2)

outliers_andel_Z1 <- outliers_Z1/length(Z1)
outliers_andel_Z2 <- outliers_Z2/length(Z2)
```

Dette giver en andel af outliers i ```Z1``` på ```r outliers_andel_Z1``` og i ```Z2``` på ```r outliers_andel_Z2```. Dette svarer til cirka $0,7~ \%$ outliers i både ```Z1``` og ```Z2```, hvilket stemmer overens med punkt 4.

Der er altså ikke noget evidens imod, at ```Z1``` og ```Z2``` skulle være normalfordelt. Tværtimod underbygger beregningen af deres middelværdi, at de er _standard_ normalfordelte.

Hernest kigges der på histogrammerne for ```Z1``` og ```Z2```. På figur \@ref(fig:bmhist) sammenlignes histogrammerne for henholdsvis ```Z1``` og ```Z2``` med densiteten for standard normalfordeling.

```{r bmhist, fig.align='center', fig.cap= "Sammenligning af histogrammerne for henholdsvis Z1 (a) og Z2 (b) med densiteten for standard normalfordeling."}
x <- seq(-4, 4, length.out = 100)
y <- dnorm(x, mean = 0, sd = 1)

par(mfrow = c(1,2))

hist(Z1, main = "(a)", ylab = "Frekvens", prob = TRUE)
lines(x, y, col = "blue")

hist(Z2, main = "(b)", ylab = "Frekvens", prob = TRUE)
lines(x, y, col = "blue")

```

Det ses på histogrammerne, at de tilnærmelsesvist er normalfordelte, samt at de har en middelværdi på $\approx 0$ og en standardafvigelse på $\approx 1$, og derved er  ```Z1``` og ```Z2``` tilnærmelsesvist standard normalfordelte.

Desuden er det også en konsekvens af Box-Muller-transformationerne, at ```Z1``` og ```Z2``` er uafhængige. Dette undersøges på figur \@ref(fig:korrelationstest).

```{r korrelationstest, fig.align='center', fig.cap='Graf for uafhængigheden mellem Z1 og Z2.'}
plot(Z2 ~ Z1, main = "",
    col = rgb(red = 0, green = 0, blue = 0, alpha = 0.25),
    pch = 1, cex = 0.5)
```

Udover at eftervise uafhængigheden af ```Z1``` og ```Z2``` grafisk, kan det også undersøges ved hjælp af funktionen ```cor``` i R, som giver en værdi for korrelationen af de to talsekvenser.
Korrelationen af to variabler udregnes ved formlen

$$
cor(X, Y) = \frac{cov(X, Y)}{sd(X) \cdot sd(Y)}
$$
Hvor $cov$ angiver kovariansen og $sd$ angiver standardafvigelsen.

Korrelation fortæller, hvor stor en lineær sammenhæng der er mellem to variabler. Denne værdi ligger i intervallet $[-1, 1]$, hvor $-1$ påviser en perfekt negativ lineær sammenhæng, $1$ påviser en perfekt positiv lineær sammenhæng, og $0$ viser, at der ingen lineær sammenhæng er.

```{r}
cor(Z1, Z2)
```

Et resultat på afrundet $-0.00048$, hvilket næsten er 0, viser altså også at der ingen lineær korrelation er mellem ```Z1``` og ```Z2```, hvilket giver en indiktion for, at de er uafhængige.

## Simulering i R {#pop-sim}

I dette afsnit gives eksempler på, hvorledes der kan udføres simuleringer af forskellige fordelinger, således disse fordelinger kan anvendes til statistisk inferens. Afsnittet er primært baseret på kilden [@Rsimulation].

I dataanalysen vil de viste fordelinger bruges som populationer, og stikprøver udtages så ved hjælp af forskellige funktioner. Idéen med denne fremgangsmåde er, at der derved udtages stikprøver fra teoretisk uendelige fordelinger.

Nogle af de fordelinger, der kan simuleres i R, er normal-, binomial-, uniforme-, beta- og gammafordelinger. Alle disse fordelinger har forskellige karateristika, som påvirker hvilke værdier de kan antage. Der vil nu vises eksempler på, hvordan disse fordelinger simuleres i R, og hvordan resultatet vil se ud.

### Normalfordeling

I R simuleres en normalfordeling ved funktionen ```rnorm```. At en stokastisk variabel, X, er normalfordelt med middelværdi $\mu$ og standardafvigelse $\sigma$ skrives $X \sim N(\mu, \sigma)$. Fordelingen på figur \@ref(fig:normal) bliver genereret ud fra en middelværdi på $0$ og en standardafvigelse på $1$. 

```{r normal, echo = FALSE, fig.align='center', fig.cap = "Densitetskurven for en standard normalfordeling."}
x <- seq(-4, 4, length.out = 100)
y <- dnorm(x, mean = 0, sd = 1)
plot(x, y, type = "l", main = "", ylab = "Densitet", xlab = "x")
```

### Uniform fordeling

En uniform fordeling er defineret ud fra et interval, som der skal genereres et givent antal værdier indenfor. Alle disse værdier har lige stor sandsynlighed for at blive genereret. Jo større $n$ er, des mere uniformt bliver histogrammet. At en stokastisk variabel, X, er uniformt fordelt i intervallet $(a, b)$ skrives $X \sim \text{unif}(a, b)$. På figur \@ref(fig:unifordeling) ses densitetskurven for en uniform fordeling.

```{r unifordeling, fig.align='center', fig.cap = "En uniform fordeling, hvor alle tal har lige stor sandsynlighed for at optræde i intervallet [0 , 1]."}
x <- seq(1, 2, length.out = 100)
plot(x = NA, type = "n", xlim = c(0, 1), ylim = c(0, 1.5), ylab = "Densitet", xlab = "x")
abline(h = 1)
```

### Binomialfordeling

En binomialfordeling bliver genereret ud fra sandsynligheder med to udfald, succes eller fiasko. I den binomiale fordeling gives en _size_ som er størrelsen af et eksperiment, samt en sandsynlighed for succes, angivet med _prob_. For hver gentagelse af eksperimentet optælles andelen af successer, som minimalt kan være $0$ og maksimalt $1$. At en stokastisk variabel, X, er binomal fordelt  med $n$ antal eksperimenter og en sandsynlighed for sucess på $p$, skrives $X \sim \text{binom}(n, p)$. Andelen af successer fra hvert eksperiment illustreres herefter i et histogram.

På figur \@ref(fig:binomfordeling) vises densitetskurven af en binomialfordeling.

```{r binomfordeling, echo = FALSE, fig.align='center', fig.cap = "En binomialfordeling med size = 6 og en sandsynlighed for succes på prob = 1/3."}
x <- c(0,1,2,3,4,5,6)
y <- dbinom(x, size = 6, prob = 1/3)
plot(x, y, xlim = c(0,6), ylim = c(0.023, 0.6), main = "", ylab = "Densitet", xlab = "x", type = "h")
```

### Skæve fordelinger

En skæv fordeling er kendetegnet ved, at størstedelen af observationerne er samlet omkring visse værdier, mens de resterende observationer fordeler sig ud til kun én af siderne. De resterende observationer kaldes for "halen" af fordelingen, og alt efter retningen af dem, kaldes den skæve fordeling for "venstreskæv" elller "højreskæv".

#### Betafordeling

En skæv fordeling kan være en betafordelingAt en stokastisk variabel, $X$ er betafordelt med parametrene $\alpha$ og $\beta$ skrives $X \sim Beta(\alpha, \beta$. I en betafordeling angiver $\alpha - 1$ andelen af sucesser, og $\beta - 1$ angiver andelen af fiaskoer, [@TDSBeta].

En venstreskæv og en højreskæv betafordeling vil nu simuleres og vises på figur \@ref(fig:betafordeling), hvor $\alpha = 8$ og $\beta = 2$ for den venstreskæve, mens $\alpha = 2$ og $\beta = 8$ for den højreskæve.

```{r betafordeling, fig.align='center', fig.cap = "To betafordelinger med henholdsvis alfa = 8 og beta = 2, og alfa = 2 og beta = 8."}
x <- seq(0, 1, length.out = 100)
y1 <- dbeta(x, shape1 = 2, shape2 = 8)
y2 <- dbeta(x, shape1 = 8, shape2 = 2)

plot(x, y1, type = "l", main = "", ylab = "Densitet", xlab = "x", col = "blue")
lines(x, y2, col = "red")
legend(0.35, 3, legend=c("alfa = 2, beta = 8 ", "alfa = 8, beta = 2"),
       col=c("blue", "red"), lty=c(1,1,1), cex=0.8)
```

#### Gammafordeling

Gammafordelingen er ligeledes en skæv fordeling. Funktionaliteten af gammafordelingen er at forudsige ventetiden til den $\alpha$'te begivenhed. Tæthedsfunktionen for gammafordelingen, er beskrevet med to forskellige parametersæt, henholdsvis $(k,~\theta)$ og $(\alpha,~\beta)$. I denne rapport vil der fremefter blive anvendt  parameterne $(\alpha,~\beta)$. Tæthedsfuktionen er givet ved $f(x)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$, hvor $\alpha > 0$ er _shape_, $\beta > 0$ er _rate_ og $x \in ( 0, \infty)$. At en stokastisk variabel, X, er gammafordelt med _shape_ $\alpha$ og _rate_ $\beta$, skrives $X \sim Gamma(\alpha, \beta)$, [@Gammadist]. På figur \@ref(fig:gammafordeling) er vist tre forskellige højreskæve gammafordelinger.

```{r gammafordeling, fig.align='center', fig.cap = "Tre gammafordelinger hvor beta = 1 og alfa på henholdsvis 1, 5 og 10."}
x <- seq(0, 25, length.out = 100)
y1 <- dgamma(x, shape = 1, rate = 1)
y2 <- dgamma(x, shape = 5, rate = 1)
y3 <- dgamma(x, shape = 10, rate = 1)
plot(x, y1, type = "l", main = "", ylab = "Densitet", xlab = "x", col = "blue")
lines(x, y2, col = "red")
lines(x, y3, col = "green")
legend(20, 0.8, legend=c("alfa = 1", "alfa = 5", "alfa = 10"),
       col=c("blue", "red", "green"), lty=c(1,1,1), cex=0.8)
```

### Repetitioner

I nogle statistiske metoder er det nødvendigt at arbejde med flere stikprøver end kun én enkelt. Med funktionen ```replicate``` i R, kan der genereres adskillige nye stikprøver ud fra den oprindelige. Processen med _replicate_ vil nu vises. Først ses en stikprøve med tre observationer.

```{r echo = FALSE}
stik <- rnorm(3, 0 ,1) #Den oprindelige stikprøve
stik_matrix <- matrix(stik, nrow = 3)
colnames(stik_matrix) <- c("Oprindelig stikprøve")
rownames(stik_matrix) <- c("Observation 1", "Observation 2", "Observation 3")
stik_matrix
```

_Replicate_-funktionen vil nu bruges. I alt replikeres der fem gange, hvilket betyder, at der bliver oprettet fem nye stikprøver. I forbindelse med denne process anvendes funktionen ```sample```, som er funktionen der _resampler_, enten med eller uden tilbagelægning fra stikprøven. I dette eksempel bruges funktionen med tilbagelægning. Fra den oprindelige stikprøve udtrækkes en observation, som indsættes i en ny stikprøve. Observationeren bliver derefter lagt tilbage i den oprindelige stikprøve, dette medfører at den samme observation kan udtages flere gange. Grunden til dette er, at stikprøven anses for at være repræsentativ for population, og to observationer af den samme værdi derfor ikke er utænkelig.

```{r}
ny_stik <- replicate(5, { #Der replikeres 5 gange
  sample(stik, size = 3, replace = TRUE)
})
ny_stik
```

Funktionen har nu resamplet fem nye repræsentative stikprøver udfra den oprindelige. Denne metode og dens anvendelser vil der kigges nærmere på kapitel \@ref(boot-afsnit).
