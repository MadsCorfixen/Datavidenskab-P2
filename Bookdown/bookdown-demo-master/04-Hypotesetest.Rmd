```{r include=FALSE, MESSAGE = FALSE}
library(mosaic)
```

# Hypotesetest

I det følgende afsnit introduceres hypotesetests, samt hvorledes disse anvendes til at drage konklusioner for en population, ved at opstille to hypoteser. 

En hypotesetest baserer sig på det videnskabelige princip om falsificering. Der opstilles en indledende formodning om en population, kaldet nulhypotesen $H_0$, og en alternativ, modsat hypotese $H_1$. Er den indledende formodning ikke korrekt, må den alternative hypotese være gældende.

Ved en hypotesetest undersøges, hvorvidt der er en difference mellem observerede værdier og forventede værdier, hvis $H_0$ er sand.

Sandsynligheden for at der er en difference er stor, eftersom der arbejdes på en stikprøve og ikke selve populationen, og derfor benyttes et mål for, hvornår differencen er _for_ stor, kaldet signifikansniveauet, $\alpha$.

En hypotesetest viser sandsynligheden for mulige udfald, for på den måde at undersøge, hvorvidt $H_0$ kan forkastes, [@HvorforHYPO].

Et mål for, hvor usandsynlig en observeret værdi er, hvis $H_0$ er sand, kaldes for en teststørrelse.

For at kunne bestemme, om en difference mellem en observeret og forventet værdi er signifikant, benyttes en signifikanstest. En signifikanstest er en metode til at finde teststørrelsen og undersøge, om den er signifikant eller ej.

Teststørrelsen findes ofte som antallet af standardafvigelser, den observerede værdi, $\hat \theta$, ligger fra den forventede værdi $\theta_0$.

At $\hat \theta$ ligger mere end $3$ standardafvigelser fra $\theta_0$, er højst usandsynligt, da $\hat \theta$ i så fald er en outlier i populationen. I et sådan tilfælde er $\theta_0$ højst sandsynligt ikke populationens korrekte værdi.

En illustration af teststørrelsens betydning ved en normalfordeling kan ses på Figur \@ref(fig:figur-Hypotesetest).

```{r, figur-Hypotesetest, out.width='75%', fig.align='center', fig.cap = "Teststørrelsens indflydelse på nulhypotesen", echo = FALSE}
knitr::include_graphics('images/HippoHyppo.jpeg')
```

Derudover benyttes testtørrelsen til at udregne _p_-værdien, som er sandsynligheden for at få en teststørrelse, der er lige så eller mere ekstrem, hvis $H_0$ er sand.

Værdien af teststørrelsen påvirker _p_-værdien på den måde, at når teststørrelsen bliver mere ekstrem, falder _p_-værdien. Jo mindre _p_-værdien er, des mindre stoles på $H_0$, og hvis _p_-værdien er mindre end signifikansniveauet, $\alpha$, forkastes $H_0$. Er _p_-værdien derimod større end $\alpha$, er der ikke belæg for at forkaste $H_0$ - dette betyder dog ikke, at $H_0$ givetvis er sand.

Normalt arbejdes der med et signifikansniveau på $5\%$, $\alpha=0.05$. Dog er der intet fast signifikansniveau og det kunne lige såvel være $10\%$ eller $1\%$. Betydningen heraf diskuteres kort sidst i afsnittet under fejltyper, [@ASTA-HYPO].

## Hypotesetest for middelværdier {#t-test}

__FIXME__ Mangler kilder

I dette afsnit gennemgåes fremgangsmåden for, hvordan en hypotesetest kan bruges til at bestemme middelværdien for en population. En sådan hypotesetest kaldes en t-test.

Først er der nogle antagelser, der skal være opfyldt, for at t-testen ikke giver misvisende resultater.

1. Stikprøven er repræsentativ for populationen.
2. Variablen er kvantitativ.
3. Stikprøveudtagning er udført med tilfældighed.
4. Populationen er normalfordelt.

Herefter opstilles hypoteserne. Nulhypotesen, $H_0: \mu = \mu_0$ og den alternative hypotese $H_1: \mu \neq \mu_0$.

Dernæst sættes et signifikansniveau, $\alpha$, der vurderer med hvilken sikkerhed $H_0$ forkastes.

Derefter beregnes den observerede teststørrelse, $t_{obs} = \frac{|\bar y - \mu_o|}{\text{se}}$, hvor $\text{se} = \frac{s}{\sqrt{n}}$.

Til slut findes _p_-værdien, og på baggrund af denne, bliver nulhypotesen enten forkastet eller ej.

__Eksempel__

Der vil nu vises et eksempel på en t-test.

```{r include=FALSE}
set.seed(1)
n <- 10
forventet_middelvaerdi <- 0
xdata <- rnorm(n, forventet_middelvaerdi, 1)
x_middelvaerdi <- mean(xdata)
```

Figur \@ref(fig:hist10) viser en stikprøve af ```r n``` observationer med en middelværdi på ```r x_middelvaerdi```, udtaget fra en standardnormalfordelt population med en forventet middelværdi på $0$.

```{r hist10, echo=FALSE, fig.align='center', fig.cap = "Histogram over 10 simulerede standardnormalfordelte tal."}
hist(xdata, main = NULL,
     ylab="Frekvens",
     xlab="Værdi")
```

I kodestykket nedenfor gennemgås den beskrevede fremgangsmåde for en t-test.

```{r}
n <- 10

forventet_middelvaerdi <- 0 # Forventet middelværdi

middelvaerdi <- mean(xdata) # Middelværdi

standardafvigelse <- sd(xdata) # Standardafvigelsen

estimeret_standardfejl <- standardafvigelse/sqrt(n) # Estimeret standardfejl

t_obs <- (abs(middelvaerdi-forventet_middelvaerdi))/estimeret_standardfejl
  # Observeret teststørrelse

alpha_halve <-  1 - pdist("t", q = t_obs, df = n-1, plot = FALSE)

p <- 2 * alpha_halve
```

```{r include=FALSE}
# Viser hvis forventede mean havde været lavere
forventet_meanx <- -0.5
t_obsx <- (abs(middelvaerdi-forventet_meanx))/
  estimeret_standardfejl # Observeret teststørrelse
x_p = 2 * (1 - pdist("t", q = t_obsx, df = n-1, plot = FALSE))
```

Eftersom *p*-værdien er ```r p``` $> \alpha=0.05$, forkastes $H_0$ ikke. Havde det derimod været en forventet værdi på ```r forventet_meanx```, ville *p*-værdien blive ```r x_p``` $< \alpha=0.05$, hvilket vil medføre, at $H_0$ forkastes og det vil formodes, at $H_0$ ikke er korrekt for populationen.

## Fejltyper

Der er risiko for to primære fejl når der foretages en hypotesetest. Den første, type-I fejl, er hvor $H_0$ forkastes, men i virkeligheden er sand, og den anden, type-II fejl, er hvor $H_0$ accepteres, men i realiteten er falsk.

En af de primære årsager til disse typer fejl er, hvor signifikansniveauet bliver sat, da dette i nogle tilfælde har stor betydning for, hvorvidt en hypotese bliver forkastet eller ej.

Sandsynligheden for type-I fejl er lig med det valgte signifikansniveau - i de fleste tilfælde $5\%$. Sandsynligheden for type-II fejl er derimod ikke let at præcisere. Dog er der stor sandsynlighed for type-II fejl, hvis den virkelige sandhed er tæt på nulhypotesen og lille, hvis den er langt fra. Ligeledes har stikprøvens størrelse indflydelse, eftersom meget data mindsker risikoen for type-II fejl, hvor der er større risiko ved mindre data, [@Fejltyper].

```{r, figur-typefejl, out.width='75%', fig.align='center', fig.cap = "Tabel over fejltyper", echo = FALSE}
knitr::include_graphics('images/Typefejl.png')
```

FIXME: Skriv om 'styrken' (jævnfør arbejdsblad fra 26/3)

## Uparret t-test for ikke-normalfordelte stikprøver {#t-test2}

I nogle tilfælde er det ikke muligt at overholde alle de pågældende antagelser, som beskrevet i afsnit \@ref(t-test), der hører til en t-test når der udføres statistisk inferens. Når dette sker, kan det ikke altid antages, at resultaterne er retvisende. I dette afsnit vil det vises, hvad der kan ske, hvis stikprøverne ikke er normalfordelte, når der arbejdes med en uparret t-test. 

Først udtrækkes to stikprøver på de skæve populationer i afsnit \@ref(pop-sim).

```{r}
#To stikprøver trukket fra populationerne
Stik1 <- sample(pop_hs, size = 100)
Stik2 <- sample(pop_vs, size = 100)
```

```{r figurpop1, fig.align="center", echo=FALSE, fig.cap= "Stikprøve fra højreskæv betafordeling hvor n = 100, alpha = 2, beta = 8"}
hist(Stik1, main = "", ylab = "", xlab = "")
```

```{r figurpop2, fig.align="center", echo=FALSE, fig.cap= "Stikprøve fra venstreskæv betafordeling hvor n = 100, alpha = 8, beta = 2."}
hist(Stik2, main = "", ylab = "", xlab = "")
```

Nulhypotesen er at $H_0: \mu_1 - \mu_2 = 0$ og den alternative hypotese er $H_1 : \mu_1 - \mu_2 \neq 0$. Denne nulhypotese undersøges ved hjælp af en uparret t-test. Udfra stikprøverne udføres der to-sidet uparret t-test, ved hjælp af den indbyggede funktion `t.test`.

```{r}
#t.test ud fra de to stikprøver
t1 <- t.test(Stik1, Stik2, alternative = "two.sided", mu = 0, conf.level = 0.95)
#Det tilhørende konfidensinterval fra t.testen
konfinterval <- t1$conf.int
```

Udfra t-testen fåes et konfidensinterval på [```r t1$conf.int```]. Med et signifikansniveau på $5\%$, vil den observerede forskel mellem populationernes middelværdier ligge i dette interval i $95\%$ af tilfældene, ifølge t-testen. Dækningsgraden af dette konfidensinterval kan undersøges ved at trække nye stikprøver fra populationerne, i alt $10,000$ gange. Dette gøres ved at udregne den nye observerede forskel i middelværdi og sammenligne med konfidensintervallet. Hvis den observede forskel er indeholdt i konfidensintervallet er det en succes, ellers er det en fiasko.

```{r ronni, fig.cap= "Kode test"}
#Undersøg dækningsgraden af konfidensintervallet ved udtræk af nye stikprøver.
res <- replicate(10000, {
  x1 <- sample(pop_hs, 100)
  x2 <- sample(pop_vs, 100)
  
  mean_diff <- mean(x1) - mean(x2)
  
  konfinterval[1L] <= mean_diff & konfinterval[2L] >= mean_diff
})
tf <- table(res)
```

```{r piettest, fig.align="center", fig.cap= "Dækningsgraden af konfidensintervallet, illusteret ved andel af succeser og fiaskoer ", echo=FALSE}
labels <- c("Fiasko","Succes")
pctf <- tf[1]/sum(tf)*100
pctt <- tf[2]/sum(tf)*100
labels <- paste(labels, c(pctf, pctt))
labels <- paste(labels, "%", sep = "")
pie(x = tf, labels, main = "", col = c("brown3", "cornflowerblue"))
```

Det fremgår fra figur \@ref(fig:piettest), at i ```r pctf```$\%$ af tilfældene ligger den observerede forskel i middelværdierne uden for konfidensintervallet. Dette stemmer ikke overens med antagelsen om, at $5\%$ burde ligge udenfor.

### Undersøgelse af variation i dækningsgraden

Konfidensintervallet og den tilhørende dækningsgrad fra figur \@ref(fig:piettest), stammer fra to stikprøver udtaget tilfældigt fra to populationer. Derfor vil dækningsgraden af konfidensintervallerne variere, alt efter hvor repræsentativ stikprøverne er for populationerne. Denne variation bliver undersøgt i dette afsnit. 

Foretages den uparrede t-test på fire nye stikprøver fremgår der store variationer i dækningsgraden af konfidensintervallet, som ses på figur \@ref(fig:piettest4).

```{r piettest4, fig.align="center", fig.cap= "Variation af dækningsgrad af konfidensintervaller på fire forskellige stikprøver", echo=FALSE}
set.seed(7)
x <- 4
PlotMatrix <- matrix(0, nrow = 0, ncol = 2)
res_1 <- replicate(x, {
  Stik_1 <- sample(pop_hs, size = 100)
  Stik_2 <- sample(pop_vs, size = 100)
  
  t_1 <- t.test(Stik_1, Stik_2, alternative = "two.sided", mu = 0, conf.level = 0.95)
  konfinterval <- t_1$conf.int
  
  res_2 <- replicate(1000, {
    x_1 <- sample(pop_hs, size = 100)
    x_2 <- sample(pop_vs, size = 100)
  
    mean_diff <- mean(x_1) - mean(x_2)
  
    konfinterval[1L] <= mean_diff & konfinterval[2L] >= mean_diff
  })
  
  tf_t <- table(res_2)
  pct_f <- tf_t[1]/sum(tf_t)*100
  pct_t <- tf_t[2]/sum(tf_t)*100
  
  PlotMatrix <- rbind(PlotMatrix, c(pct_f, pct_t))
  return(PlotMatrix)
})
PlotMatrixFinal <- t(matrix(res_1, 2, 4))
par(mfrow=c(2,2))
labels1 <- c("Fiasko","Succes")
pctf_1 <- PlotMatrixFinal[1,1]
pctt_1 <- PlotMatrixFinal[1,2]
labels1 <- paste(labels1, c(pctf_1, pctt_1))
labels1 <- paste(labels1, "%", sep = "")
pie(x = PlotMatrixFinal[1,], labels1, main = "", radius = 1, col = c("brown3", "cornflowerblue"))
labels2 <- c("Fiasko","Succes")
pctf_2 <- PlotMatrixFinal[2,1]
pctt_2 <- PlotMatrixFinal[2,2]
labels2 <- paste(labels2, c(pctf_2, pctt_2))
labels2 <- paste(labels2, "%", sep = "")
pie(x = PlotMatrixFinal[2,], labels2, main = "", radius = 1, col = c("brown3", "cornflowerblue"))
labels3 <- c("Fiasko","Succes")
pctf_3 <- PlotMatrixFinal[3,1]
pctt_3 <- PlotMatrixFinal[3,2]
labels3 <- paste(labels3, c(pctf_3, pctt_3))
labels3 <- paste(labels3, "%", sep = "")
pie(x = PlotMatrixFinal[3,], labels3, main = "", radius = 1, col = c("brown3", "cornflowerblue"))
labels4 <- c("Fiasko","Succes")
pctf_4 <- PlotMatrixFinal[4,1]
pctt_4 <- PlotMatrixFinal[4,2]
labels4 <- paste(labels4, c(pctf_4, pctt_4))
labels4 <- paste(labels4, "%", sep = "")
pie(x = PlotMatrixFinal[4,], labels4, main = "", radius = 1, col = c("brown3", "cornflowerblue"))
```

På grund af den uventet store variation i dækningsgraden, vil det være interessant at undersøge den gennemsnitslige dækningsgrad for konfidensintervallet. Dette gøres i nedenstående kodestykke, ved at beregne den gennemsnitlige dækningsgrad af $100$ forskellige konfidensintervaller. 

```{r}
x <- 100
res_1 <- replicate(x, {
  Stik_1 <- sample(pop_hs, size = 100)
  Stik_2 <- sample(pop_vs, size = 100)
  
  t_1 <- t.test(Stik_1, Stik_2, alternative = "two.sided", mu = 0, conf.level = 0.95)
  konfinterval <- t_1$conf.int
  
  res_2 <- replicate(1000, {
    x_1 <- sample(pop_hs, size = 100)
    x_2 <- sample(pop_vs, size = 100)
  
    mean_diff <- mean(x_1) - mean(x_2)
  
    konfinterval[1L] <= mean_diff & konfinterval[2L] >= mean_diff
  })
  
  tf_t <- table(res_2)
  # Gennemsnitslig procentvise afvigelse
  pct_fiasko <- tf_t[1]/sum(tf_t)*100
})
gns_pct <- sum(res_1)/x
gns_pct
daekningsgrad <- 100 - gns_pct
```

Det fremgår, at i ```r gns_pct```$\%$ af tilfældene ligger den observerede forskel i middelværdierne uden for konfidensintervallet. Altså har konfidensintervallet fra t-testen kun en sand dækningsgrad på ```r daekningsgrad```$\%$, hvor den antagede er $95\%$.  Dette stemmer stadig ikke overens med antagelsen om, at $5\%$ burde ligge udenfor. Konklusionen er derfor, at en t-test udført på ikke-normalfordelte stikprøver ikke nødvendigvis giver et retvisende resultat.
